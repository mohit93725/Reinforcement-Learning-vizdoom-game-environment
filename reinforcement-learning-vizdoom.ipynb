{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install vizdoom","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:18:16.467255Z","iopub.execute_input":"2025-06-25T18:18:16.467934Z","iopub.status.idle":"2025-06-25T18:18:22.814401Z","shell.execute_reply.started":"2025-06-25T18:18:16.467873Z","shell.execute_reply":"2025-06-25T18:18:22.813679Z"}},"outputs":[{"name":"stdout","text":"Collecting vizdoom\n  Downloading vizdoom-1.2.4-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vizdoom) (1.26.4)\nRequirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from vizdoom) (0.29.0)\nRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from vizdoom) (2.6.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (3.1.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.13.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vizdoom) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vizdoom) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vizdoom) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vizdoom) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vizdoom) (2024.2.0)\nDownloading vizdoom-1.2.4-cp311-cp311-manylinux_2_28_x86_64.whl (28.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.1/28.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: vizdoom\nSuccessfully installed vizdoom-1.2.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Import Libraries**","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym\nimport vizdoom as vzd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:18:33.639624Z","iopub.execute_input":"2025-06-25T18:18:33.640000Z","iopub.status.idle":"2025-06-25T18:18:33.643702Z","shell.execute_reply.started":"2025-06-25T18:18:33.639981Z","shell.execute_reply":"2025-06-25T18:18:33.643114Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Create Game Environment**","metadata":{}},{"cell_type":"code","source":"#create vizdoom game environment function\ndef create_vizdoom_env():\n    # initiate the game\n    game = vzd.DoomGame()\n    # load game configurations\n    game.load_config(vzd.scenarios_path + \"/basic.cfg\")\n\n    # Disable rendering to avoid animation\n    game.set_window_visible(False)\n    game.set_render_hud(False)\n    game.set_render_crosshair(False)\n    game.set_render_weapon(False)\n    game.set_render_decals(False)\n    game.set_render_particles(False)\n    game.set_screen_resolution(vzd.ScreenResolution.RES_160X120)\n\n    game.init()\n    return game","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:51:47.384043Z","iopub.execute_input":"2025-06-25T18:51:47.384375Z","iopub.status.idle":"2025-06-25T18:51:47.389433Z","shell.execute_reply.started":"2025-06-25T18:51:47.384350Z","shell.execute_reply":"2025-06-25T18:51:47.388729Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"**Store Experiences for Learning**","metadata":{}},{"cell_type":"code","source":"# ReplayBuffer class for storing the past experiences\nclass ReplayBuffer:\n    def __init__(self, capacity = 100000, history_len = 4):  # max 100000 frames can be saved, and 4 frames to define a state\n        self.capacity = capacity\n        self.frame_queue = deque(maxlen = history_len)\n        self.history_len = history_len\n        self.buffer = deque(maxlen = capacity)\n        \n# reset history and store first frame in the frame_queue which is also the default state\n    def reset_history(self, first_frame: torch.Tensor):\n        \n        self.frame_queue.clear()\n        for _ in range(self.history_len):\n            self.frame_queue.append(first_frame.clone())\n\n# this function first check the new frame shape and then append the state, action, reward, next_state, done in a list\n    def push(self, new_frame: torch.Tensor, action: int, reward: float, done: bool):\n\n        assert new_frame.shape == (1, 84, 84)\n\n        state_stack = torch.cat(list(self.frame_queue), dim =0) # concatenate frames along the dimension 0 which is along the batch size\n# add new frame in frame queue and then create a new stack of next state\n        self.frame_queue.append(new_frame) \n        next_state_stack = torch.cat(list(self.frame_queue), dim=0)\n\n        self.buffer.append((state_stack, action, reward, next_state_stack, done))\n# this function takes some samples as experience from batches\n    def sample(self, batch_size: int):\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        return list(states), list(actions), list(rewards), list(next_states), list(dones)\n\n    def __len__(self):\n        return len(self.buffer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:18:43.027326Z","iopub.execute_input":"2025-06-25T18:18:43.027568Z","iopub.status.idle":"2025-06-25T18:18:43.034262Z","shell.execute_reply.started":"2025-06-25T18:18:43.027551Z","shell.execute_reply":"2025-06-25T18:18:43.033656Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**Preprocess the input states**","metadata":{}},{"cell_type":"code","source":"def preprocess(state):\n    #cv2 takes image in the shape (H,W,C)\n    img = np.moveaxis(state, 0, -1)  # input image: (3, 240, 320), output image: (240, 320, 3)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    resized = cv2.resize(gray, (84, 84))\n    return torch.tensor(resized, dtype=torch.float32).unsqueeze(0) / 255.0  # shape: (1, 84, 84)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T18:18:45.391881Z","iopub.execute_input":"2025-06-25T18:18:45.392514Z","iopub.status.idle":"2025-06-25T18:18:45.396451Z","shell.execute_reply.started":"2025-06-25T18:18:45.392490Z","shell.execute_reply":"2025-06-25T18:18:45.395941Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Model architecture**","metadata":{}},{"cell_type":"code","source":"class CNNDQN(nn.Module):\n    def __init__(self, action_space):\n        super(CNNDQN, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(4, 32, 8, 4),      # input image: (1,4, 84, 84) \n            nn.BatchNorm2d(32),\n            nn.SiLU(),\n\n            nn.Conv2d(32, 64, 4, 2),     \n            nn.BatchNorm2d(64),\n            nn.SiLU(),\n\n            nn.Conv2d(64, 64, 3, 1),    \n            nn.BatchNorm2d(64),\n            nn.SiLU()\n        )\n\n        # automatic feature size detection\n        with torch.no_grad():\n            dummy = torch.zeros(1, 4, 84, 84)\n            flat_dim = self.conv(dummy).view(1, -1).size(1)\n\n        self.value   = nn.Sequential(\n            nn.Linear(flat_dim, 512), nn.SiLU(),\n            nn.Linear(512, 1)\n        )\n        self.advantage = nn.Sequential(\n            nn.Linear(flat_dim, 512), nn.SiLU(),\n            nn.Linear(512, action_space)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:        \n        z = self.conv(x)                                       \n        z = z.flatten(1)                                      \n        v = self.value(z)                                      \n        a = self.advantage(z)                                  \n        q = v + a - a.mean(dim=1, keepdim=True)              \n        return q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T19:16:11.296174Z","iopub.execute_input":"2025-06-25T19:16:11.296784Z","iopub.status.idle":"2025-06-25T19:16:11.313104Z","shell.execute_reply.started":"2025-06-25T19:16:11.296760Z","shell.execute_reply":"2025-06-25T19:16:11.312496Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Training setup**","metadata":{}},{"cell_type":"code","source":"EPISODES       = 2000\nBATCH_SIZE     = 32\nGAMMA          = 0.99\nLR             = 1e-4\nEPS_START      = 1.0\nEPS_MIN        = 0.10\nEPS_DECAY      = 0.995      \nTARGET_SYNC    = 10          \nGRAD_CLIP      = 5.0\n\n\n\ndef train():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    game       = create_vizdoom_env()                 \n    n_actions  = game.get_available_buttons_size()\n# we initialize two networks: (1) policy and (2) target. Target network will help in evaluating the \"running\" policy network\n    policy_net  = CNNDQN(n_actions).to(device) \n    target_net  = CNNDQN(n_actions).to(device)\n    target_net.load_state_dict(policy_net.state_dict()) \n    target_net.eval()\n\n    optimiser   = optim.Adam(policy_net.parameters(), lr=LR)\n    loss_fn     = nn.SmoothL1Loss()\n    buffer      = ReplayBuffer(capacity=100_000)\n\n    epsilon     = EPS_START\n\n    for ep in range(EPISODES):\n        game.new_episode()\n\n# preprocessing the game state to get the first frame which will be stored as a reset frame\n        first_frame = preprocess(game.get_state().screen_buffer) \n        buffer.reset_history(first_frame)\n\n        total_reward, steps = 0.0, 0\n\n        while not game.is_episode_finished():\n            steps += 1\n\n            # build current (4,84,84) stack and add batch dim → (1,4,84,84)\n            state_stack = torch.cat(list(buffer.frame_queue), dim=0).unsqueeze(0).to(device)\n\n            # epsilon-greedy action\n            if torch.rand(1).item() < epsilon:\n                action_idx = torch.randint(0, n_actions, (1,)).item()\n            else:\n                with torch.no_grad():\n                    action_idx = policy_net(state_stack).argmax().item()\n\n            # act (repeat = 1 frame)\n            reward = game.make_action([1 if i == action_idx else 0 for i in range(n_actions)])\n\n            # basic shaping / clipping\n            reward = max(min(reward, 1), -1)\n\n            done = game.is_episode_finished()\n            if not done:\n                new_frame = preprocess(game.get_state().screen_buffer)  # (1,84,84)\n            else:\n                new_frame = torch.zeros_like(first_frame)               # dummy frame\n\n            # store transition (buffer handles stacking internally)\n            buffer.push(new_frame, action_idx, reward, done)\n            total_reward += reward\n\n            \n            if len(buffer) >= BATCH_SIZE:\n                states, actions, rewards, next_states, dones = buffer.sample(BATCH_SIZE)\n\n                states      = torch.stack(states).to(device)           \n                next_states = torch.stack(next_states).to(device)\n                actions     = torch.tensor(actions).long().to(device)\n                rewards     = torch.tensor(rewards).float().to(device)\n                dones       = torch.tensor(dones).float().to(device)\n\n                # current Q(s,a)\n                q_pred = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n\n                # Double-DQN target Q-value\n                with torch.no_grad():\n                    next_actions = policy_net(next_states).argmax(1, keepdim=True)\n                    q_next = target_net(next_states).gather(1, next_actions).squeeze()\n                    q_tgt  = rewards + GAMMA * q_next * (1 - dones)\n\n                # optimise\n                loss = loss_fn(q_pred, q_tgt)\n                optimiser.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(policy_net.parameters(), GRAD_CLIP)\n                optimiser.step()\n\n        \n        if ep % TARGET_SYNC == 0:\n            target_net.load_state_dict(policy_net.state_dict())\n            print(f\"Ep {ep:4d} │ steps {steps:3d} │ reward {total_reward:6.2f} │ epsilon {epsilon:.2f}\")\n\n        # ε decay\n        epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n\n    game.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T19:16:12.490237Z","iopub.execute_input":"2025-06-25T19:16:12.490774Z","iopub.status.idle":"2025-06-25T19:21:19.159938Z","shell.execute_reply.started":"2025-06-25T19:16:12.490750Z","shell.execute_reply":"2025-06-25T19:21:19.158832Z"}},"outputs":[{"name":"stdout","text":"Ep    0 │ steps 300 │ reward -300.00 │ epsilon 1.00\nEp   10 │ steps   7 │ reward  -5.00 │ epsilon 0.95\nEp   20 │ steps 300 │ reward -300.00 │ epsilon 0.90\nEp   30 │ steps 300 │ reward -300.00 │ epsilon 0.86\nEp   40 │ steps 165 │ reward -163.00 │ epsilon 0.82\nEp   50 │ steps 300 │ reward -300.00 │ epsilon 0.78\nEp   60 │ steps 300 │ reward -300.00 │ epsilon 0.74\nEp   70 │ steps  29 │ reward -27.00 │ epsilon 0.70\nEp   80 │ steps  23 │ reward -21.00 │ epsilon 0.67\nEp   90 │ steps 300 │ reward -300.00 │ epsilon 0.64\nEp  100 │ steps 300 │ reward -300.00 │ epsilon 0.61\nEp  110 │ steps   6 │ reward  -4.00 │ epsilon 0.58\nEp  120 │ steps 300 │ reward -300.00 │ epsilon 0.55\nEp  130 │ steps 192 │ reward -190.00 │ epsilon 0.52\nEp  140 │ steps  10 │ reward  -8.00 │ epsilon 0.50\nEp  150 │ steps  26 │ reward -24.00 │ epsilon 0.47\nEp  160 │ steps 300 │ reward -300.00 │ epsilon 0.45\nEp  170 │ steps 300 │ reward -300.00 │ epsilon 0.43\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3364925475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/3564014733.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRAD_CLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_max_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_div_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}